# Aula 21: Overview of Evaluation Methodology in ML

Video: https://www.youtube.com/watch?v=7bMbolxKeic

## Porque avaliar? 

É normal que os nossos modelos falhem, é importante percebermos a sua verdadeira performance, e é importante destinguir os nossos modelos no contexto em que são aplicados. Não podemos permitir que uma carro automático falhe de vez em quando, mas podemos permitir que um detetor de spam falhe 2 ou 3 vezes a cada 100 emails. Qual é a perforamance que diz que um modelo é util?.

Temos o nosso training set e o nosso test set (ainda temos um validation set). O validation set serve para medirmos a performance do modelo DURANTE o processo de treino. Este procedimento é chamado de cross validation.

## Como avaliar?

1. Avaliação intrinseca: medir a nossa performance em isolação utilizando algumas medidas para computar a nossa métrica automáticamente.

2. Avaliação extrinseca: Já mudei o meu previsor, como é que isto está a afetar os resultados (estou a receber mais dinheiro, mais cliques, está a funcionar?)

## Diferentes dominios

É importante notar que as métricas de avaliação dependem de cada dominio, portanto também é importante repararmos o que está a ser utilizado no mercado, se faz ou não sentido. Trabalhar com pessoas do meio. Temos que PERCEBER os nossos dados. Isto pode acabar por ser dificil dependendo das áreas.

Algumas aplicações até utilizam HUMANOS como classificadores, ter pesssoas a avaliar o nosso modelo (exemplo: tradução). Claro que usar humanos é sempre dificil, caro e não são livres de erro.