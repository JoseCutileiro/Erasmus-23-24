# Aula 20: Boosting

Video: https://www.youtube.com/watch?v=Nj66jH4jrNE

## Revisões de ensembles

Vimos que os ensembles podem-nos dar mais confiança e robustez no que toca à qualidade dos nossos modelos (em caso particular vimos as random forestss que são ensembles para decision trees). Também vimos bagging.

Hoje: Construir ensembles iterativamente

## Qual é a ideia então? 

Assumios que temos um classificador ou um regressor e que ele faz alguns erros no nosso conjunto de treino. A ideia é tentar aprender um modelo que limpe estes erros. Isto é o que chamamos boosting.


Modelos -> reziduals -> Modelos -> Reziduals -> ...

Depois pegamos nos nossos modelos e juntamo-los

Algoritmos:

1. AdaBoost: Especifico para problemas de classificação
2. Gradient Boosting: Generalização do AdaBoost (funciona para classificação e também para regressão)

## Ada boost

Step 1: Modelo sobre os dados

Step 2: Ver que pontos errámos -> aumentar o peso destes pontos

Step 3: Repetir quantas vezes achares necessário

## Gradient boosting

Ideia: Gradualmente adicionar sub-modelos para minimizar a nossa loss function

Pseudo code
```py

F0 = dummyModel()

for m in range(M):
    for x,y in zip(training,set):
        ri = pseudoResidual(yi, Fm-1(xi))
    
    trainsubmodel hm on the pseudo residuals

    Fm += Fm-1.Append(hm)

return Fm
```

Geralmente usam trees lá dentro

O que é o psudo-residual: É o gradiente negativo da nossa loss function

O que significa adicionar um submodelo ao nosso modelo?

Adicionar o nosso h à nossa loss function, normalmente multiplicamos com um learning rate.

## No scikit learn

parametros importantes do gradient boosting:

1. ensemble size (nr_estimators)
2. learning rate

No random forest o nr_estimators não implica overfitting, mas no gradient boosting este parametro afeta o nosso overfitting. Isto porque estamos a aperfeiçoar o nosso modelo aos dados diretamente.

## Implementações do gradient  boosting

1. scikitlearn (python)
2. XGBoost (várias linguagens)
3. H20 (Várias linguagens)

## Gradient boosting

Este algoritmo começou a ficar famoso nos ultimos anos, conseguem ter resultados muito bons nas competições de AI (exemplo: Kaggle)