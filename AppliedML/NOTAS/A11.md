# Aula 11: Linear Classifiers

Video: https://www.youtube.com/watch?v=eTxdyMAAiaw

## Binary classification tasks

Dizemos que a classificação é binária se tivermos duas classes possiveis no output.

## Classificadores lineares:

1. Linear classifier

Temos uma função de scores

Score =  w * x

(vetor de pesos e vetor de features)
Também pode aparecer doutra maneira, mas a forma se cima não é mais que um caso geral.

Score = w * x + b 

## (exemplo):

Queremos classificar documentos como tendo uma opinião boa ou má 

Temos um texto (INPUT): 

This book is fantastic

Temos multiplas palavras:

[garbage, this, like, fantastic]

Temos pesos associados às palavrass:

[-3, 0.3, 1.2, 3]

Temos um bag of words (input intermédio): 

CountVectorizer 

[0,1,0,1]

E agora tentamos prever o input que nos foi dado. 

## Visualização no plano

Pontos e uma linha

## Como treinar classificadores lineares? 

Basicamente o nosso objetivo é chegar ao W correto, a questão é como. Pode ser com o percetrão com o naive bayes, com o support vecotr machines...

## Percetrão 

1. Começar com o nosso W com tudo a zeros.
2. repetir: classificar o resultado com o nosso W atual, atualizar se o resultado foi mau (ver pseudo codigo)

```py
w = zeros()

repeat N times:
    for (xi, yi) in the training set:
        score = w * xi

        if (errado):
            if (y é positivo):
                w += xi
            else:
                w -= xi
return w
```

Limitação do percetrão: Os classificadores lineares só conseguem resultados bons se os dados forem linearmente separáveis. XOR problem

## Como criar um classificador linear?

```py

class LinearClassifier(obj):

    def predict(self, w):
        score = x.dot(self.w)
    
        if (score >= 0.0):
            return positive_class
        
        else:
            return negative_class
    
    def predict_all(self,X):
        scores = X.dot(self.w)
        out = numpy.select([scores >= 0.0, scores < 0.0],   [postive_class,negative_class])

        return out


class Perceptron(LinearClassifier):

    def fit(self, X, Y):
        n_features  = X.shape[1]
        self.w = numpy.zeros(n_features)

        for i in range(NUM_ITERS):
            for x, y in zip(X,Y):
                score = self.w.dor(x)

                if (score <= 0 and y == positive_class):
                    self.w += x
                
                if (score > 0 and y == negative_calss):
                    self.w -= x
```

Este é apenas um dos LinearClassifiers, existem mais e já implementados no scikit learn (Percetron, logistic regression, linearSVC)

No scikit learn o W é o model.coef_

