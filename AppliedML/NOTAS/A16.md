# Aula 16: Linear regression and regularization

Video: https://www.youtube.com/watch?v=WbXWq0evGLY

## Minimizar squared errors 

least squares aproach for linear regression, usamos o squared error outra vez

aplicar o SGD para o least square loss

f(w) = (w * x - yi)^2 

grad(f(w)) = 2 * (w*x - y) * x

Mais uma vez atualizamos o peso com o calculo deste gradiente. (rever o widrow-hoff).

Underfitting | Ideal | Overfit 

goodness of fit: o modelo aprendido deve descrever bem os exemplos dos dados de treino

regularização: O modelo deve ser o mais simples possivel

## (RIDGE REGRESSION) Como manter o modelo simples?

1. Magnitude dos pesos: Penalizar pesos grandes (L2 regularizer) provavelmente é o regularizer mais conhecido em ML

Ideia: Loss_function + alpha * regularizer 

alpha -> serve para controlar o balanço entre overfit e overfit. Se o alpha for mais baixo a penalização é mais pequena e o modelo tem mais risco de overfit, se for muito alto o modelo será muito simples e terá mais porbabilidade de underfit.

Ridge regrssion -> é o modelo com a least_squares_loss + alpha * regularizer. 

SGD aplicado na ridge regression: O novo termo é chamado muitas vezes de weight decay.

Existem outras opções para além desta regularização: Lasso regression (usa a L1 norm). Podemos ainda fazer tanto a L1 e a L2 regularização e isso dá a ElasticNet (precisa de mais um parametro adicional). A L1 regularization tende a apresentar soluções esparsas. Ao contrario do L2. Automaticamente a L1 faz uma feature selection AUTOMATICA (muito bom isto). O que faz com que possamos retirar/excluir estas features. O que faz com que o nosso algoritmo seja muito mais eficiente. Neste caso se o nosso alpha for muito alto, mais features vão tender a desaparecer se for baixo menos features vão desaparecer. 

Temos sempre que encontrar o balanço entre utilizade, otimização, underfit e overfit... Aqueles problemas comuns desta área.

Existe uma explicação para o porque do lasso dar coeficientes a zero. https://stats.stackexchange.com/questions/151954/sparsity-in-lasso-and-advantage-over-ridge-statistical-learning



