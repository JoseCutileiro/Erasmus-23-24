# Aula 17: Logistic regression

Video: https://www.youtube.com/watch?v=OMdIU7iNduI

## Logistic regression (intro)

Apesar do nome, vamos nos focar em classificação, començando com classificação binária.

Relembra:

score = w * x 

Large positive score => x pertence à classe positiva

Large negative score => x pertence à classe negativa

Perto de zero => Não consegue dizer nada com muita confiança

A confiança não é diretamente interpretavel, podemos ter algum modelo que trate disto como probabilidades? Simmm => logistic regression model.

## Logistic regression (detailed)

É um método para treinar classificadores lineares que nos devolve outputs probabilisticos. Como é que podemos obter estas probabilidades. Aqui é que entra um grande amigo de ML, o softmax. Só aqui uma nota engraçada, a função SIGMOID é um caso particular da funçáo softmax para classes binárias. 

Sigmoid: 

input: x

output: 1 / (1 + exp(-x))

Este é um dos classificadores mais conhecidos em ML, também pode ser chamado de "Maximum entropy classifier" <=> logistic regression.

## Intrepretar os resultados 

O resultado pode ser chamado de "log odds". Definição de odds -> quanto mais provavel é o resultado ser positivo do que negativo? 

odds = p / (1-p)

Só aqui uma nota da função sigmoid, repara que: 

1 - sigmoid(x) == sigmoid(-x)

Esta igualdade facilita muito no processo de treino de um modelo

P(label | input) = sigmoid(label * score)

(se passarmos a label para 1 e -1, fica trivial obter o resultado invertido)

## Relembrar: Principio da máxima verosimilhança

Nos modelos probabilisticos, podemos treinar modelos selecionando os parametros que atribuem uma alta probabilidade aos dados. No nosso caso estes parametros são o nosso vetor de pesos. Objetivo: ajustar os w para que o output label tenha uma probabilidade alta.

Likelihood function:

L(w) = P(y1 | x1) * P(y2 | x2)... P(ym | xm)

Queremos maximizar esta função, que é o mesmo que minimizar a LOG LOSS (que também é chamado de binary cross entropy loss). (há uma mini demonstração que não vou colocar aqui)

Loss(w,x,y) = log( 1 + exp(-y * (w * x)))

## Regularização

1. goodness of fit: O que aprendemos deve classificar corretamente os nossos exemplos do training set
2. regularization: O classificador deve-se manter o mais simples possivel. Para isso aplicamos regularização. Para a Logistic regression aplicamos os mesmos regularizadores que a linear regression (L1, L2 ...). 

## Objetive function (logistic regression)

Há duas...

(1 / N) * loss + (lambda / 2) * regularizer: Controlar o lamda para saber se damos mais ou menos foco ao regularizador.

(C / N) * loss + (1 / 2) * regularizer: Controlar o C para saber se damos mais ou menos foco ao nosso conjunto de treino.

Esta função objetivo é convexa! O gradient descent funciona bem (encontramos o global minimum caso encontremos um minimo)

## Notas duvidosas: 

Maximizar a nossa likelihood <=> minimizar a log loss

Apesar de se chamar logistic regression, este é um classificador.

O grande aspeto positivo deste modelo é que os resultados têm uma leitura probabilistica, o que pode ser muito positivo em alguns aspetos.
