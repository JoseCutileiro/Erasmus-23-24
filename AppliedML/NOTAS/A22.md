# Aula 22:  Evaluation of Classifiers and Regressors

Video: https://www.youtube.com/watch?v=d-Bgg-2CRTw

## Setup tipico

Avaliação intrinseca: Quão bons os outputs são

(pode ser feito olhando simplesmente para a ground_truth)

## Métricas comuns para modelos de classificação

1. accuracy = correct / total
2. error = incorrect / total
3. Confusion matrix (aqui entra o conceito de TP, FP, TN, FN), isto é crucial para alguns problemas (exemplo: problemas com classes não balanceadas) e levanos a algumas medidas menos comuns -> Precision, Recall
4. Precision = TP / (TP + FP)
5. Recall = TP / (TP + FN)
6. F1 score = (2 * P * R) / (P + R)
7. Também é comum utilizar outras medidas -> Por exemplo na medicina existe a sensibilidade e a especificidade
8. Sensibilidade = TP / (TP + FN) (==recall)
9. Especificidade = TN / (TN + FP)
10. Ainda existem os rate -> True postivie rate, e false positive rate
11. TPR = TP / (TP + FN)
12. FPR = FP / (FP + TN)

## Métricas comuns para modelos de regressão

Aqui não podemos ver se falharmos ou não, dada a continuade do espaço de solução seria basicamente impossivel acertar nos valores, para isso aparecem outras medidas: MSE e MAE

1. MSE: Mean squared error -> Se tivermos uma instância que falha muito esta métrica dará muita importância (devido ao quadrado)
2. MAE: Mean absolute error -> Se tivermos outliars não ligamos tanto

Temos ainda um problema das escalas. Para resolver este problema aparece o coefficient de determinação R2, esta métrica não depende da escala (se tiver valor 1 -> score perfeito, se tiver valor 0-> regressor burro). Vê na net a formula desta medida dado que é chata de colocar aqui. https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html