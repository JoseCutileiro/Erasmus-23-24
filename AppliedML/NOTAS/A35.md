# Aula 35: Introduction to Dimensionality Reduction

Video: https://www.youtube.com/watch?v=synM02mm9Wg

Nota: isto ainda é considerado unsupervised learning

## Para que serve?

1. Melhorar interpretação 
2. Descobriar alguns padrões => Representation learning
3. Aprender algumas coisas sobre o dataset

## Dimensionality reduction 

Queremos reduzir a dimensionalidade do dataset, temos um dadaset com muitas colunas aqui o objetivo é reduzir o número de colunas. Claro que o nosso objetivo é fazer isto sem perda de informação. 

Porque é que queremos fazer isto? 

1. Mais fácil interpretar um dataset com menos dimensões
2. Reduzir o overfitting => Largar a maldição da dimensionalidade
3. Otimização dos próprios algoritmos

## PCA: Principal component analysis

Objetivo => ter um novo espaço de dimensões

Intuitivamente => Tentar encontrar a direção na qual o nosso dataset está a ir, isto dá-nos um dos vetores => PRIMEIRO PRINCIPAL COMPONENT || Depois removemos este PC e repetimos o primeiro processo. Agora temos o SEGUNDO PRINCIPAL COMPONENT  ...


Podemos repetir isto o número de vezes que queremos

Temos um novo sistema de dimensões, mas para que é que isto serve? 

Podemos reduzir as dimensões para o número que quisermos. Os principal components estão ordenados por ordem de relevância, e são escolhidos com a variancia da dimensão

## Behind PCA: Singular value decomposition

====================================================

1. PCA pode ser implementado com uma simples fatorização de matrizes com uma técnica chamada SVD (singular value decomposition)

M = U EPSILON V*

U tem dimensão mxm

EPISLON tem dimensão mxn

V* tem dimensão nxn

Contudo isto é muito problemático e não escala para matrizes muito grandes como é o nosso caso

====================================================

2. Implementações nas bibliotecas => low-rank factorization

é um bocadinho mais dificil, para usar no scikit learn => truncatedSVD

====================================================

3. Outra forma => fatorização das matrizes com SGD (stocastic gradient descent)

Loss function: alternating least squares

====================================================