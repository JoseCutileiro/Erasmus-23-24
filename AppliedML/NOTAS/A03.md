# Aula 3: ENSEMBLES

video: https://www.youtube.com/watch?v=TNkBkWkz6Jg

## Introduction to Ensembles

A ideia chave é usar multiplos modelos em vez de um, isto permite aproveitar os aspetos positivos de cada um dos modelos. Exemplo simples:

Um dos modelos tem accuracy 60% e o outro 70%, assumimos que os erros dos modelos são independentes (o que não é realista na prática mas é bonito). Claro que juntanto muitos modelos, se os erros forem sempre independentes, a accuracy depois daria bem.

## Como implementar os ensembles? 

## VOTING

### Voto por maioria 

eg.

1. Modelo X  ---> vota 1
2. Modelo Y  ---> vota 0
3. Modelo Z  ---> vota 1

Resultado: 1

### Voto por média

Cada modelos diz que o output é X com probabilidade P, depois a ideia é fazer a média e o que tiver melhor resultado sai no final.

### Stacking

Usa o output de cada um dos modelos, e usa estes outputs como features para um novo modelo.

## Programar: 

Não é muito complicado de implementar uma coisa do genero de raiz, mas podemos usar com o scikit learn, já tem uma coisa que funciona bem.

## E para a regressão?

Usamos as mesma técnicas, e.g média ou stacking.
O por maioria não faria tanto sentido.

## Training ensebles

A ideia é que os modelos sejam diversos. Como tornar os modelos diversos? Uma maneira simples é cada um dos modelos focasse numa parte do training set. Ou fazer pequenas alterações no training set para cada um dos modelos, também funciona bem.

## BAGGING (bootstrap aggregating)

Training set original é partido (com reposição, ou seja cada split do set pode ter registos que aparecem noutro split)

Amostras do training set são alimentadas a cada um dos modelos, e no fim combinar os resultados

## SUBSET OF FEATURES

feature bagging (sem reposição, as features que foram retiradas num dos datasets não seram retiradas no outro)

A mesma ideia, mas para colunas do dataset.

