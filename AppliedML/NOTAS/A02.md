# Aula 2: DECISION TREE

link: https://www.youtube.com/watch?v=fUoOhjMF9zg

# VIDEO

## Decision tree

Formalmente uma decision tree é um programa que vai basear as suas
previsões dependendo de vários IF e ELSE (binário)

Exemplo

```
        A
       / \
      B   C
     / \ / \
```

Se A vai para B se B escolhe algo ...

Nós internos: Decisões  

Leaf node: Output value

## Que funções matemáticas podemos retirar? 

As regiões são todas com angulos retos

1. Fácil de entender e visualizar
2. Pode ter limitações

## Como é que funciona o algoritmo? 


Vemos qual é a feature mais útil, e vamos colocando
no topo, repetimos isto quantas vezes quisersmos (ou até ao número 
de features no máximo).

Exemplos de algoritmos: ID3, C4.5, CART ...

# PSEUDO-CODE
```
def TrainDecisionTreeClassifier(X,Y, depth=0):
    if (all ouputs in Y are the same):
        return leaf with that output
    
    if (depth > maxdepth):
        return leaf with majoraty class
    
    F = allFeatures(X)

    # Decide the best feature
    for fi in F:
        Xi,Yi <- subset onde F = fi
        Ti <- TRAIN(Xi,Yi)
    
    return tree that splits on F
```

# USEFUL FEATURE? 

É útil se o subgrupo é homogénio

Exemplos:
1. majoraty class sum
2. information gain
3. Gini impurity (é o default do scikit-learn)

# Homogéneo?

## Majority class 

somar as frequências

## Information gain 

Baseado na entropia dos subsets

Relembra: Entropia a 0 se todos forem o mesmo valor, entropria máxima se todas as classes aparecem o mesmo número de vezes

Objetivo: Fazer com que a entropia seja o mais próximo de 0 possível, isso significa que os subsets são bastante homogeneos.

## Treshold

É importante definir o treshhold como deve ser também, 
depois de escolheres a variavel mais util, escolhes o valor que mais uma
vez, faz com que exista menos entropia no subset.

Exemplo:

```
        Best_Var < Best_Treshold
            /          \
        True          False
        /                 \
    Class_A              Class_B

```


## Regression trees

Relembra: Regression é para preveres outputs numéricos, funciona exatamente da mesma forma mas em vez de preveres a class prevês o número

## Problemas

Relembra:

1. Underfit: Demasiado simple
2. Overfit: Demasiado complexo

Nas decision trees, isto é visto pela DEPTH. Se tivermos muito pouca estamos a olhar para poucas features o que faz com que o modelo seja muito simples, no outro lado se tiveres demasiado simplesmente estás a decorar o dataset. O que faz com que tenha piores resultados no TEST SET. (os resultados no TRAIN SET são cada vez melhores)

## Notas finais:

## Datasets
https://archive.ics.uci.edu/datasets









        

