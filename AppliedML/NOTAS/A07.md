# Aula 7: Feature selection

Video: https://www.youtube.com/watch?v=jldtA2VA6Ro

## Qual é a ideia? 

Algumas das features que temos são realmente uteis para encontrar os resultados que queremos. Mas podemos cair no erro de acreditar que todas as features são uteis. Então a ideia é tentar limpar algumas das features. Isto diminui o risco de overfit e pode fazer com que o modelo acabe por generalizar melhor.

Feature selection é mesmo isto, tentar encontrar um subset das features originais. 

## Metodos

1. Metodos de filtragem: Originalmente começamso com o set de todas as features, depois escolhemos um subset, fazemos o nosso algoritmo e vemos a performance, depois manualmente decidimos o que acharmos melhor.
2. Wrapper methods: Inicialmente começamos com todas as features também, depois a ideia é fazer o mesmo que os metodos de filtragem mas a escolha do subset está incluida é feita com a ajuda do modelo (loop em que adaptamos o subset ao modelo). No final avaliamos a performance
3. Embedded methods: O mesmo que os wrapper methods mas a avaliação da performance também está incluida no loop e ajuda a escolher o melhor subset

## Associar a feature e o output

Nota: Só aqui uma lembrança --> false predictor

Posto isto de lado, a ideia é ver que variaveis estão correlacionadas com o output, a ideia é ter uma função de classificação (podem haver várias dependendo se é para classificação ou regressão)

## Funções de classificação: 

1. Mutual information

Temos duas váriavies discretas (!= continuas), X e Y, l(X,Y) = somatorio de todos os X e Y de p(x,y) * log (p(x,y)  / p(x)*p(y))

2. F-score (baseado no teste anova) (estatico)

3. Qui-Quadrado (baseado no teste do qui-quadrado) (estatico)

4. Self-check (o que acontece se X e Y forem independentes? => Naive bayes assumption)

## Feature selection no scikit-learn

1. Seletores -> SelectKbest, SelectPercentile
2. Feature scoring function -> f_classif, mutual_info_classif, chi2, mutual_info_refression, f_regression
   
## Problemas comuns:

Cada feature é considerada em isolação, podem haver features que funcionam apenas se tiverem combinadas. Exemplo do problema do XOR, se tivermos duas variaveis X1 e X2 e Y (output). Saber que X1 é 0 ou 1 em nada nos vai ajudar a prever o output. Apenas com a ajuda de X2 podemos prever o output na perfeição. 

## Solução

1. Brute force: Fazer TODOS os subsets possiveis dado um conjunto de variaveis. Isto não funciona na prática... (devido à explosão exponencial que vem do fator combinatório da questão, 2^N). Só aqui uma notinha se não perceberem o porque de 2^N (ou está ativo ou não está ativo). Poranto dado que isto não funciona, nós queremos de alguma forma aproximar este algoritmo. 
2. Wrapper method (Sequential forward selection): Começar vazio e ir enchendo com as variaveis que mais ajudam até já não ajudar mais ou não ajudar quase nada. Tem as suas vantagens: Não adiciona features redundantes, é muito simples. Mas também têm as suas desvantagens: Pode ser impossivel de apanhar as features a funcionar em conjunto (não consegue resolver o XOR por exemplo), mesmo este aproach greedy pode ser pesado se houverem muitas features (apesar de exponencialmente menos pesado que o outro, mas estamos a falar de modelos de ML que por si só têm uma complexidade elevada), é sub-optimal, ou seja este algoritmo não irá encontrar o subset otimo para resolver o problema.
   
## Embedded feature selection 

O próprio algoritmo de ML trata deste problema, por exemplo as DECISION TREES (fazem feature selection a cada depth). Alguns modelos lineares também fazem isto utilizando a regularização. Alguns exemplos de regularização são, a L1 (LASSO), a L0 a L2 ... (isto vai ser falado)