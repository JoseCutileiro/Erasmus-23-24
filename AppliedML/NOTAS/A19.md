# Aula 19: Multiclass linear classifiers

Video: https://www.youtube.com/watch?v=hMq-d969GHM

## Until now: 

Até agora aprendemos modelos capazes de lidar com  duas classes, queremos agora elevar um bocadinho o nosso nivel e aprender modelos capazes de lidar com mais classes. 

Ideias: 

1. Partir o problema em modelos mais simples, e treinar um modelo binário para cada parte do problema.
2. Modificar diretamente o algoritmo para que seja capaz de lidar com várias classes.

## Ideia 1: Reduzir o problema

1. One-versus-rest("long jump"): A ideia é treinar um classificador binário para cada classe, (uma sendo a positiva e as outras todas negativas, e repetir isto para todas as classes). Depois pegar no resultado mais forte

2. one-verus-one("football league"): Treinamos uma classificador para cada par (existem N * (n-1) / 2) classificadores. Quando formos classificar, escolhemos a classe que tiver mais vitorias.
   
### Example

Temos tres frutas: Maça, Laranja, Manga

One versus rest: 
1. apple X orange + mango
2. orange X apple + mango
3. mango X appls + orange

One versus one. 
1. apple X mango
2. appls X orange
3. orange X mango
(nota: apesar de nao se notar neste exemplo, o one versus one tem um crescimento exponencial, o que faz com que geralmente seja um aproach pouco eficaz)

## No scikit learn

1. OneVsRestClassifier
2. OneVsoneClassifier

Existem dois aproaches para lidar com multiclass no scikit learn, o primeiro é utilizar os classificadores proprios da biblioteca que já resolvem o problema das multiclasses também. Outro é no caso de quereres lidar com o teu próprio classificador linear, para isso tens que fazer um ad-on com estas funções definidas lá em cima. 

## Ideia 2: Melhorar a solução

Só aqui uma relembrada de um binary logistic regression. A ideia é converter o nosso score numa probabilidade. A loss function é a loss loss (binary cross entropy loss). Lembram-se qu utilizávamos a nossa amiga sigmoid. 

Agora: Passamos para a softmax (que é compativel com multiclass)

### Softmax

Pseudo codigo
```py

def softmax(scores):
    expscores = np.exp(scores)
    return expscores / sum(expscores)


# Isto pode dar bostique devido a overflows
# A ideia passa então para subtrair o máximo, evitanto overflows

def softmax(scores):
    exp_x = np.exp(x - np.max(x))
    return exp_x / sum(exp_x)
```

## Como treinar? 

Na binary logistic regression: log loss (binary)

Para multiclass: cross-entropy loss (que é muito semelhante mas usa o softmax em vez da sigmoid)

O resultado é exatamente o mesmo

grande chance de estar correta => loss pequena
grande chance de não ser esta class => muita loss

## Multiclass  logistic regression no scikit learn:

LogisticRegression(multi_class='multinomial')