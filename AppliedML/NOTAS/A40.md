# Aula 40: Recurrent Neural Networks

Video: https://www.youtube.com/watch?v=4JA0jXRsWKY

## Ideias chave

RNN não são uma coisa exclusivamente, é uma familia de arquiteturas para processar sequências com machine learning. 

Operação básica:

-> recebemos uma sequência de inputs, temos um cvv (continuos value vector) e devolvemos uma classificação, regressão ou uma nova sequência.

Exemplo:

INPUT: This is not a very good

Percorrer o input token a token (no caso de ser um texto) e ir alimentanto informação do passado ao token seguinte.

Alternativa: averagind the states, ir a todos os tokens e fazer uma média entre os estados de cada token para devolver o output

## Aplicações:

1. Name entity recognition 
2. Sequence generation

# Como funcionam as RNNs? 

##  RNN (simple RNN // Elman RNN)

É muito semelhante a uma FFNN, não é muito usada devido ao problema do vanishing gradient. É muito complicado fazer com que o modelo se lemmbre de coisas que aconteceram no inicio da sequência. A solução para este problema é feita já noutras RNNs -> gating 

## gating

Permite controlar a sequencia de informação de forma mais cuidada

## LSTM (long short term memory)

É uma RRN que usa gates, de resto é muito semelhante a uma RNN normal

Função de ativação: Sigmoid

De onde vem o long-term: forget gate - cancelar o estado anterior ou deixar passar? 


## GRU: Gated recurrent units

É mais simples e mais eficiente para so computadores, é um concurrente da LSTM