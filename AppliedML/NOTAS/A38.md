# Aula 38: Word embeddings

Video: https://www.youtube.com/watch?v=vEOhiyoBUqY

## Relembrar: Tarefas de classificação de documentos

Exemplo: 

"Estou ansioso que a Leonor venha!" - Positive

"Não quero nada ter este exame." - Negative

## Deep learning in NLP 

Cada vez mais o deep learning está a ficar popular em tarefas de NLP. É bom a fazer tarefas complexas, é compativel com transfer learning

## Transfer learning

Aproveitar modelos aprendidos anteriormente para alimentar ou pre-treinar novos modelos

## Representar documentos em machine learning

A mais fácil e comum => bag-of-words, permite representar, esta representação apesar de util não é muito otima, isto porque cada palavra adiciona uma coluna (uma nova dimensão). E para além disto não capta semelhanças entre palavras exemplo: batata vs cenoura == batata vs correr.

Para resolver estes problemas aparecem os word embeddings.

## Training word embeddings

end-to-end training: Aprendemos embedingss especializados

pre-training: Aprendemos embeddings genéricos para depois usar em diferentes tarefas

## Embeddings no KERAS 

```py

model = Sequential()
model.add(Embedding(1000,64))

# O que é que isto significa?
# Temos um vocabulário com 1000 palavras e 
# queremos embeddings de 64 dimensões
```

Ideia no pre-training word embeddings: As palavras que têm um comportamento semelhante, contextos semelhantes <=> palavras semelhanças. Exemplo: Café e chá.

Objetivo: Tentar criar embeddings que captem estas semelhanças

co-occurrence matrix: Matriz que vê as palavras que aparecem perto umas das outras. Agora a ideia é fatorizar esta matriz

## GloVe: matrix-based word embedding training method

1. Minimizar uma loss function com a matriz de coocurrence
2. Palavras que aparecem menos vezes têm pesos menores.

Depois de ter as embeddings temos o que precisamos para perceber as semelhanças entre as palavras. Podemos ainda usar o t-SNE e o PCA para visualizar estas semelhanças!

