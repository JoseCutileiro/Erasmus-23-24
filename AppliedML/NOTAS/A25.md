# Aula 25: Training Neural Networks

Video: https://www.youtube.com/watch?v=OfE5U8lVIb0

## O que significa treinar NN?

É em tudo muito semlhante aos modelos lineares, temos uma função que queremos otimizar (objective function). Na altura vimos um algoritmo que nos permitia otimziar os paremtros dos modelos lineares -> SGD

## SGD: stocastic gradient descent

Apesar de ser um algoritmo utilizado em modelos lineares, é compative com modelos não lineares como as NN. Repetimos este processo um número de vezes suficientes e a magia acontece, no caso das NN, os pesos de cada unidade serão ajustados o melhor possivel

Relembra algumas losses:

1. Log loss (binary classification)
2. Cross entropy (multi classification)
3. Squared error (regression)

## Output da NN 

Temos os nossos pesos W

Temos o nosso input X

Calcular o z = dot(W,x)

Calcular o h = sigmoid(z) (sigmoid pode ser qualquer função de ativação)

Repetir isto para cada uma das layers, utilizando o output da layer anterior (h) como input (x) para a layer seguinte.

Este processo de repetir as contas faz com que cada layer no fundo seja uma computação aninhada noutra (composição de funções). Isto faz com que seja possivel utilzar a regra da cadeia para calcular os gradientes.

## Regra da cadeia: 

grad(f(g(x))) = grad(f(g)) *  grad(g(x))

Isto faz com que seja possivel termos um grafo de computação. Para calcularmos as coisas mais facilmente.

Nota: Os gradientes são calculados em respeito à loss. (Ver o minuto 10:57 para teres informação visual).

Isto é CRUCIAL para fazer a backpropagation. Cada gradiente reutiliza passos do gradiente anterior para fazer as suas calculações. A cena boa é que as bibliotacas já resolvem estes problemas por nós.