# Aula 33: K-means Clustering

Video: https://www.youtube.com/watch?v=Ge6Djzgh2ac

## K-means

1. Deve ser o algoritmo de cluster mais famoso
2. Ideia informal: Encontrar os K clusters tal que estão mais perto do centroid (vetor médio)

## Loss function

residual sum of squares

L(S) = ...

O nosso objetivo é encontrar a partição S que minimiza esta loss function

Mas temos um problema, isso é NP hard. Ou seja não é fazivel encontrar esta solução => Surge uma aproximação

Lloyd algorithm => esta é a aproximação, e é o que é utilizado no K-means


Lloyd algorithm
```
1. Inicilizar os centroids ao calhas (centroids-> mew)
2. Cada ponto vai para o cluster S que está mais perto do centroid mew_i
3. Recomputar os centroids para cada S, como é que isto se faz? Basta calcular a média dos pontos desse centroid.
4. Repetir 2 e 3 até não convergir

return S = {S1, ..., Sk}
```

## Converge? 

Este algoritmo termina sempre? 

SIMMMM 

Explicação: 

A cada iteração, ou vamos reduzir a loss function L ou não vamos mudar nada, se não mudar nada é porque convergiu.

## Problema?

Apesar de convergir, ele pode não convergir sempre para a mesma solução. Depende da primeira inicialização aleatória. Isto invoca outro problema... Não encontra a solução otima.

Como minimizar este problema => BEAM SEARCH || K-MEANS++

## BEAM SEARCH

É trivial, basta colocar vários e escolher o melhor

## K-MEANS++

Os centroids iniciais são escolhidos de acordo com pontos do dataset, a ideia é escolher pontos afastados entre si

## Implementações do k-means

1. scikit-learn => KMeans, MiniBatchKMeans 
2. Spark
3. Dask

Nota: MiniBatchMeans => trabalha apenas com um subset do dataset , é mais rápido.

## Parametros a tunar

1. K 
2. Iniciliação dos centroids (estrategia standard => KMeans++)
3. Numero de vezes a correr
4. Como parar => numero de iterações ou convergir? 

## Uma dificuldade

Quantos clusters é que existem??? 

Apesar de parecer simples esta pergunta não é nada fácil de responder

Existe uma cursa => RSS (residual sum of squares). Esta curva é a relação entre numero de clusters e a loss. Para perceberes o problema, por exemplo se tiveres o mesmo numero de pontos de clusters tens uma loss de 0 mas isso não tem valor informativo.

Então => Como saber o numero de clusters=

1. Através do dominio
2. Heuristicas
3. Silhoutte score
4. Penalizar a loss function com o número de clusters => AIC ou BIC (tipo regualização)

Existe ainda outro método => elbow method, ver a curva RSS e ver onde é que está o nosso cotovelo.

## O que podemos retirar do K-means? 

1. Ver quais são as amostras representativas => Perto do cluster
2. Outras coisas ...

## Coisas extra:

1. Atenção ao scaling
2. Atenção à medida de distância

Tens ainda o GMM (gaussian mixture model, TODO <= ver isto)

