# Aula 18: Support vector classifiers

video: https://www.youtube.com/watch?v=BHKKmO_D0U4

## Vista geometrica de classificadores lineares

Existe uma linha (ou um plano). Que separa os nossos dados. Existe uma medida, à qual o professor chama de gama. Este gama representa a margem, ou seja o quão bem a nossa linha (ou plano) separa as nossas classes. No fundo serve para avaliar a qualidade dos nossos pesos (w).

Existe um resultado que nos diz que se as margens forem boas o classificador tende a conseguir generalizar melhor (o que acaba por ser bastante intuitivo). Isto é provado matematicamente pelo teorema de minimização de risco estrutural que tem um aspeto feioso

## Support vector classifiers (machines)

SVM: Classificadores lineares que selecionam os w que maximizam a margem. A solução depende unicamente pelas instancias que estão nas fronteiras entre as classes. Estas instancias são chamadas de "support vectors". O que isto significa é que as instancias mais interiores (longe da fronteira). Têm zero impacto na decisão do nosso separador. Na prática, isto precisa de mais algumas coisas para fazer com que isto funcione. 

1. Às vezes os nossos dados não são perfeitamente separaveis
2. Podemos ter outliars que lixam as nossas fronteiras

O nome da solução que resolve estes problemas é: soft-margin SVM, esta solução permite que existam alguns exemplos que não sejam considerados para o nosso separador final.

## SVM <-> objective function

loss = max(0, 1 - y * (w * x))

Esta é chamada a hinge loss. Se virem os plots (que não colocarei aqui por preguicite, este plot é relativamente parecido à nossa log loss de anteriormente). Mas a hinge loss não têm a primeira derivada continua, e a log loss tem. Isto dificulta a aplicação do gradient descent para a hinge loss.

## The nonlinear SVM 

Podemos generalizar o SVM para utilizar uma kernel function. Quando usamos esta kernel function permitimos que o nosso modelo aprenda modelos que não são lineares. O que nos permite aprender dados com estruturas mais complexas.

Que kernels podemos utilizar?

1. Linear: K(xi,x) = xi * x
2. Quadratic (ou qualquer polinomio): K(xi,x) = (xi * x)^2
3. Radial basis function (RBF): K(xi,x) = exp(-lamda * ||xi-x||^2)

## No scikit-learn:

sklearn.svm.LinearSVC: Mais rápido mas menos flexivel => Têm as mesmas limitações que os classificadores lineares comuns (não sabe trabalhar com dados que não são linearmente separaveis)
sklearn.svm.SVC: Mais lento mas mais flexivel (permite utilizar os nosso porprios kernals (ou outros built in))
