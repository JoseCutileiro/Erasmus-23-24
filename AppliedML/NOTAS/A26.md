# Aula 26:  Neural Network Practicalities

Video: https://www.youtube.com/watch?v=uDkSjwlCnis

## NN: Tempos 

Estes modelos são mais pesados que os modelos lineares, podem demorar horas, dias, semanas ... a correr dependendo do tamanho do modelo e do dataset. Isto porque requer muita matemática. Por isso é CRUCIAL utilizar implementações eficientes que lidam com a matematica do melhor modo que o ser humano sabe. Para isso é melhor trabalhar com bibliotecas, que já tratam desta implementação por nós

1. Google: TensorFlow
2. Facebook: PyTorch
3. Microsoft: CNTK

Estas bibliotecas já lidam com bprop, regularization ...

Ainda temos o Keras, que é mais high level. Hoje em dia o keras é uma parte do tensorflow. 

Cuda API: Falar com GPU

## Coding example with Keras: 

```py

keras_model = Sequential()

n_hidden = 3

keras_model.add(Dense(inputDim, outputDim))
keras_model.add(ACTIVATION_FUNC)

keras_model.add(Dense(outputDim, 1))
keras_model.add(ACTIVATION_FUNC)

# Definir a loss e o optimzer
keras_model.compile(binary_cross_entropy, sgd)
```

Relembrar: Optimizer => algoritmo que trata de atualizar os nossos pesos.

## Feature preprocessing

As NN são muito sensiveis ao pre processamento de features, a escala das features também afetam muito, é boa prática aplicar scaling nas features se forem para alimentar estes modelos. (exemplo: min-max scaling)

## Batch 

Processar uma feature de cada vez não é bom. Se fizermos uma de cada vez não estamos a aproveitar o paralelismo desponivel no nosso computador. Para aproveitar isto temos que definir o batch_size. Isto é uma variante do SGD clássico, chamado o SGD com minibatch. 

## Otimizar NN

É comum inicializarmos os pesos das nossas NN com valores aleatórios. Isto dá nos mais um trunfo -> multiple initizalizations. Isto permite obter vários modelos, depois é uma questão de escolher o melhor. 

Também é importante escolher um learning rate adequado, caso contrario poderemos não conseguir obter valores uteis. (nada de novo honestamente).

## Mais uma vez => lidar com overfit

1. Regularizers => como antes (l1, l2... adicionados na loss function)
2. early stopping => começar a perceber quando o nosso validation set não apresent melhorias, e terminar. 
3. dropout => desconectar unidades durante algumas etapas do treino, normalmente a cada batch.
4. data augmantation: Adicionar um noise ao conjunto de dados orignal. Segue o mesmo objetivo do dropout

## Early stopping no Keras:

```py

from keras import callbacks

cb = callback.EarlyStopping(monitor='val-loss',min_delta=0,patience=10, verbose = 0, mode='auto')

...

model.fit(...,callcack=[cb])
```

## Dropout

Importância do droupout: Não ter cada uma das unidades a resolver um problema. Quando desativamos as unidades do modelo estamos a passar a responsabilidade para todo o modelo e não apenas uma parte dele.