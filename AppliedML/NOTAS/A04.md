# Aula 4: Random forests

Video: https://www.youtube.com/watch?v=ICp2VzmdM4c

## Random forests

No fundo as random forests são ensembles de decision trees. Tal como as decision trees funcionam para classificação e regressão.

### Decision tree (revisão)

1. Ver a feature F mais util 
2. fazer a arvore com F
3. separar em subgrupos baseados em F
4. Repetir isto o nr de vezes que quiseres

(podes ver mais em A2)

Só aqui uma notazinha importante, nas decision trees normalmente quando chegamos à folha escolhemos o output. Mas o output da leaf pode ser uma probabilidade, isto é importante para os ensembles (ajuda a combinar é compativel tanto com os de maioria tanto com os de probabilidade)

(nota: MLE maximum likelihood estimator)

### Decision trees não são perfeitas... 

1. Tendem MUITOOO para overfit, são más a generalizar dado que decoram literalmente o dataset.

Os ensembles são ótimos a lidar com a redução do overfit

Random forests: Decision tree + ensemble

### Random forests

São muito boas no geral, não são o melhor modelo para uma tarefa individualmente mas conseguem obter bons resultados genéricamente. Claro que isto pode ter casos particulares que acabam por não funcionar

### Implementação 

1. Bagging (separação do training set)
2. Usa subset de features

No final

1. Classificação: Fazer médias das médias
2. Regressão: Média dos resultados (ou média ponderada)

### Hyperparameters nas random forests

1. Quantas arvores?
2. Quantas features é que cada nó considera?
3. Hyperparametros standard das decision trees (depth, feature selection criterion ...)

(relembra os critérios: information gain ou gini)

### Pros and cons

PROS

1. são muito simples
2. melhores resultados que uma single tree
3. fácil de misturar features

CONS

1. Menos interpretaveis que a single tree
2. Mais pesados que a single tree
3. Precisam de boas features (não funciona bem com imagens ou texto ou som ...)