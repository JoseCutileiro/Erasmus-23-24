# Aula 13: Collecting Data for Machine Learning and Annotating data

Video: https://www.youtube.com/watch?v=M4qrgqfKQDo

# Parte 1: Collecting Data for Machine Learning

## Dados

Usados para TREINAR e para AVALIAR. Pode server para mais que isto como jornalismo, investigação...

## Modos de explorar os dados

1. Supervises: Pares de IN e OUT, usados para prever ou classificar
2. Unsupervised: Quermos dividir o dataset em grupos (exemplo: clusters)
3. Semisupervised: Usado quando fazer label de data é muito caro

## De onde vêm os dados? 

1. Online dataset (kaggle, UCI, MNIST...)
2. Mas os dados não aparecem do céu... Provavelmente o dataset que precisas não estará em dominio publico

Como coletar dados e como anotar dados?

## Tipos de dados: 

Numerical (1,2, 5%, ...)
Categorical (male, female)
Textual
Graphical (imagem)
Audio 
...

Seleção de dados

1. TOP DOWN: O que queremos, qual é o proposito
2. BOTTOM UP: O que é que posso obter destes dados e como

Como ter os dados?

1. Databases já existentes (privados ou publicos)
2. Log file (em sensores, computadores ...)
3. Scraping from websites
4. Using APIs (exemplo de imagens: flickr)


Nota: Estar na net não é publico, treinar modelos com conteudo de autores pode dar bostique. Podem usar distribuição por URLs mas isto poderá ter outro problema pois o URL pode ficar inacessivel.

## População e representação

Queremos modelar uma população, (pessoas, imagens, sons ...). A ideia base é que queremos que a nossa amostra seja representativa e verdadeira. O que pode ser um problema e uma dificuldade. Exemplo do professor das sondagens: Recolheram mdados de maneira errada, ou focaram num grupo de pessoas não representativo ou a propria forma de coletar dados está errada e desbalanceada (isto normalmente quando lidamos com pessoas).

Areas frageis: 

1. Historia, não podemos criar mais dados
2. Material caro, é caro criar mais dados
3. Novidade, Novo cenário não sabemos bem o que será o normal

Fragilidades nos dados:

1. Tempo dos dados (há quanto tempo já foi)
2. Seleção dos dados (e.g: dados só foram recolhidos se foram ao médico, pode ser mau)
3. Demografia (e.g: dataset apenas com pessoas que usam oculos)

## Annotating data (parte 2)

Primeiro temos de produzir os dados, muitos dados têm que ser anotados manualmente. Normalmento queremos que os modelos se comportem de alguma forma como humanos, para isto precisamos de ter dados anotados por humanos (não é obrigatório mas costuma ser).

Exemplo: image tagging, object annotation.

Muitas vezes temos UI que facilitam o modo como anotamos os dados. Problema, pode levar a BIAS. O anotator é humano, pode se enganar, é preguiçoso, fica cansado...

Antes de começarmos a encher o nosso dataset é importante criar guidelines para todos os que irão anotar os dados --> um manual. Não pode haver duvidas. As anotações TÊM que ser consistentes.

Ainda falta uma coisa, encontrar as pessoas que podem anotar os nossos dados. Os dados precisam de alguma especilização de dominio? Ou nem por isso. Arranjar estudantes... Sempre mantendo a ética.

## Semi automatic annotation 

Mais uma vez pode ser perigoso, e levar a bias adicional. O próprio computador sugeste as anotações e o anotador vê se está ou não correto. 

## Crowdsourcing

Crowdsourcing: Usar muito anotatores 

MTURK

Atenção: As pessoas não são especilistas, o que implica que a nossa UI deva ser simples e a tarefa de anotação igualmente fácil.

Funciona na prática? 

Para algumas tarefas é mais que suficiente, exemplo: Está um elefante na imagem ou não. Mas para algumas tarefas mais complexas obterão maus resultados (depende muito)

Ética

É importante reparar que as pessoas que trabalham nestes estabelicimentos acabam por ter muito poucas compensasões e isso poderá ser um problema ético

Outras maneiras engraçadas de coletar dados

1. CAPTCHA
2. GAMES WITH PURPOSE
3. AMNESTY DECODERS (voluntário)

Annotation as a business (também é uma ideia)

## Controlo de qualidade

1. Double annotation (inter annotator agreement) (N annotation)
2. Mix annotation with checks
3. Inspection

e.g

1. Inter anotation score

P(a) = (number of agreements) / (number of items)
(não é muito util)

2. Chance agreement probability

k = P(a) - P(e) / 1 - P(e)

P(a) --> estarem efitivamente em acordo
P(e) --> probabilidade de estarem em acordo se fizerem tudo ao calhas

< 0.4: No good

0.4 - 0.6: average

0.6 - 0.8: good

0.8 - 1: really good

k é no good? Anotaçao imprecisa, problemas na UI,...

Varições

Cohen k se tivermos 2 annotators


Podes usar estes scores no python: StatsModels