# Aula 24: Introduction to Neural Networks

Video: https://www.youtube.com/watch?v=zsFzV91ZGrY

## Deep learning models

É a area que está mais na moda. A ideia das NN é fazer uma abstração dos inputs de modo a aprender os padrões que se escondem por detrás dos dados. E faz isto tudo automaticamente. 

O grande aspeto positivo é conseguirmos simplificar o processo de feature eng. , antigamente tinhamos de fazer coisas complicadas nos dados para podermos tirar algum tipo de informação. Com o aparecimento destes modelos mais complexos conseguimos lidar quase com os dados 'crus'. 

Pros:
1. Consegue captar relações complexas
2. São muito bons com dados 'noisy'
3. Permitem resolver problemas muito complicados e.g: tradução

Cons:
1. Computacionalmente pesados
2. Careful tweaking
3. Menos estáveis
4. Os modelos gerados são complexos -> exige mais dados
5. Resultados muitas vezes de black box

Estes modelos dominam areas com dados complexos (e.g: som ou imagens) mas quando lidamos com dados bem definidos normalmente não são utilizados.

## Reaparecimento das NN

1. Mais dados
2. GPUs
3. Mais computadores
4. Melhores estratégias (modelos em si)

## Hoje

NN básicas para classificação e regressão

## Modelos lineares (relembrar)

Limitações: Os modelos lineares náo conseguem lidar com relações que não são linearmente separaveis. O grande problema do XOR. Contudo este problema pode ser resolvido se fizermos uma transformação no conjunto de dados. A isto chamamos uma transformação não linear. Dada que esta mudança é não linear o modelo final também não será linear.

Esta é a ideia das NN. Automaticamente mudarem o espaço do dataset, de modo a que o modelo consiga aprender relações não lineares no conjunto de dados original.

## FFNN: Feed forward neural network

Este é um modelo de multiplas camadas e também é conhecido por multilayer perceptron (MLP)

Estes modelos consistem em camadas stackadas, e existem várias hidden units. A ultima camada é chamada de output layer. (Podem ser usadas para tarefa de classificação ou regressão).

Cada hidden unit tem uma parametrização diferente.

## Implemetação

A base é feita com 'dot products' contudo estes modelos são mais que um dot product. Os calculos podem ser mais otimizados com a notação matricial. Na prática daria para fazer tudo individualmente, mas este processo é mais lento.

## Funções de ativação

1. Sigmoid
2. tanh 
3. ReLU

A escolha da função de ativação é por si só um hiperparametro e tem que ser 'tunado'.

Só aqui uma notita: A função sigmoid é especifica para outputs binários, o que às vezes não é o nosso caso. A função genérica com este formato é a softmax. Relembra outra coisa: Quando implementas a softmax tens o problema do overflow, para resolver isso fazes o tal shift que já falamso à algumas aulas atrás.

Se o output for 'regression', qual é a função de ativação que devemos usar? Normalmente não utilizamos nenhuma :>

## NN no scikit learn

1. sklearn.neural_network.MLPClassifier
2. sklearn.neurral_network.MLPRegressor

Contudo estes modelos não nos dão tanta liberade como isso tudo, para isso teremos de utilizar outras bibliotecas (torch, kura, tensorflow ...)

## Poder das NN 

Universal aproximation theorem: FFNN podem aproximar qualquer função matematica (isto é verdade até para apenas uma hidden layer). Contudo isto pode não ter valro prático, se só tiveres uma hidden layer terá um grande crescimento na vertical...

Area de pesquisa dos dias de hoje: Porque é que é mais facil aprender em profundidade lateral (maior numero de hidden layers, ao em vez de profundidade na vertical??). As primeiras hidden layers lidão com a informação no nivel mais baixo. As hidden layers mais à frente lidam com dados mais abstraidos.