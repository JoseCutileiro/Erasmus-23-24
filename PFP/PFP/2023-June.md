# 2023-June

## Problem 1

- (a): Parallelism in Haskell is scheduled in Haskell Evaluation
Contexts (HECs). Each HEC has a pool of unevaluated sparks that it
takes turn completing. A spark in this sense is a computational task
that gets scheduled for execution by the Haskell runtime in one of the
available HECs.

- (b): This is not true. Every problem's potential to be parallelised is
bound by the work that it has to do sequentially.

- (c): When parallelising algorithms, for example in Haskell, we identify
which parts of the algorithm can be executed in parallel and create a spark
for each one of them. If the algorithm is a recursive one for example over
lists or over integers, the cases when the inductive data type is small might
be much faster to just solve sequentially, instead of creating many more
sparks that overflow the spark pools.

- (d): The Par monad approach is a dataflow approach to parallel programming.
When one computation is running, it can fork and execute another computation
in parallel, and communication between the parent and child computations
happen using IVars. Visually, the computations comprise the nodes of the
dataflow graph, whereas the IVars comprise its edges. In contrast to Eval,
Par does not depend on laziness and gives finer control over the
parallelisation. Also, the Par monad is implemented as a Haskell library and
is not part of the runtime, so customisations can be made to it, such as
changes in the scheduling algorithm etc.

- (e): The work is O(N) and the span is O(logN).

- (f): Futhark is an even more restricted language than NESL so that it can
deliver better performace. The main difference is that NESL allows irregular
arrays and flattens nested parallelism, which leads to polynomial space
increases. Futhark fixes this problem by not allowing irregular arrays.

- (g): In Haskell parallel processes share memory and can communicate through
this memory using IVars for example. In Erlang, processes do not share any
memory and each process manages each own stack and heap.

- (h): When a process in Erlang meets an error condition, the philosophy of
the Erlang developers is to immediately crash the process. Thanks to the
separation of memory, a crashing process cannot corrupt any other process in
the system. This way, a monitoring process can inspect the reason why the
process failed and restart it or propagate the error upwards.

- (i): Speculative parallelism extracts pieces of code from a sequential body
of code and tries to preemptively run them in parallel with the code that's
already running. Since data dependencies may disallow the actual
parallelisation of code, the thread may make assumptions about its inputs,
that if wrong will throw away the parallel work done but if correct the
result can be used by the main thread directly.

- (j): This is wrong. In general, communicating between hosts is at least an
order of magnitude slower than communicating between nodes. When programming
in Erlang, processes that need to communicate frequently should be put in the
same host as much as possible.

## Problem 2

- (a):

    ```Haskell
    divConq :: Int          -- min depth
            -> (a -> (a,a)) -- problem splitter
            -> (a -> b)     -- solver
            -> ((b,b) -> b) -- solution merger
            -> a            -- input
            -> Par b        -- output
    divConq 0 _ solve _ inp = 
        return $ solve inp
    divConq depth split solve merge inp = do
        let (xs,ys) = split inp
        xsBox <- spawn $ recurse xs
        ysBox <- spawn $ recurse ys
        xs' <- get xsBox
        ys' <- get ysBox
        return $ merge (xs',ys')
    where
        recurse = divConq (depth - 1) split solve merge
    ```

- (b):

    ```Haskell
    listSum :: Num a => [a] -> a
    listSum inp =
        divConq
            ((log2 $ length inp) / 2)
            (\l -> splitAt ((length l) / 2) l)
            sum
            (+)
    ```

- (c):

    ``` Haskell
    main :: IO ()
    main = do
        let xs = take 10000 $ randoms (mkStdGen 9471832819)
        defaultMain [ bench "listSum" (nf listSum xs) ]
    ```

- (d): parBuffer is a strategy that allows us to control task granularity of
a parallel algorithm on lists, while also allowing for the list to be lazy.
Using parBuffer is done as with any other strategy on lists: composing a map
for example with `withStrategy (parBuffer 100 rdeepseq)` will start chunking
the input stream in chunks of 100 elements and running the sequential solver
on them in parallel. parBuffer differs from parList, as it is a chunking
strategy, and it also differs from parChunk as it doesn't need to evaluate
the whole list to decide where it will chunk it. This means that parBuffer
is the only viable strategy for parallelising algorithms that run on infinite
lazy streams, as the other strategies would send the algorithm into a loop.

## Problem 3

- (a): The work is O(N) and the span is O(logN).

- (b): Because it can be parallelised well and it can be used to implement
many useful algorithms like radix sort, quick sort and lexicographic
ordering.

- (c): A segmented scan is a variation of scan, in which we partition the
input array into contiguous segments and scan each one of them separately.
Usually, the segments are denoted by a second array parallel to the input
array, which has true at the segment heads and false everywhere else.

- (d):

    ```futhark
    def op' 't (op: t -> t -> t)
               ((v1, f1): (t, bool))
               ((v2, f2): (t, bool)): (t, bool) =
        ((if f2 then v2 else op v1 v2), f1 || f2)

    def segscan [n] 't (op: t -> t -> t) (ne: t)
                       (inp: [n](t, bool)): *[n]t =
        map
            (.0)
            (scan
                (op' op)
                (ne, false)
                arr)
    ```

- (e):

    ```futhark
    def len [n] (inp: [n]i32): i32 =
        let ns = iota n
            is = map (\i -> i == 0 || (inp[i] <= inp[i - 1])) ns
        in segscan
               (\l _ -> l + 1)
               0
               (zip inp is)
    ```

- (f): Java does not have as strong a typesystem as Haskell, which means that
it can't guarantee that no side effects are happening inside the parallell
code which can mess up the intended control flow of the program.

- (g): The advantage of having a language deeply embedded in Haskell means
that the implementation gets a parser and a type checker for free from the
Haskell compiler, and also it gives the ability to the user to use it in
combination with other Haskell libraries freely. On the other hand, creating
a new language means that the syntax is friendlier and more customised so the
user can familiriase themselves with it.

- (h):

## Problem 4

- (a): The two processes run in parallel. The receiver immediately blocks
until it receives a message from the sender. As soon as the sender gets some
work to do, it sends the receiver the work and blocks until it receives the
answer. The receiver does the work and sends the answer, after which it
loops back and blocks all over again.

- (b): If we remove the exchange of acknowledgments, the sender would just
loop and send every work immediately as it creates it, which would pile them
on the receiver's mailbox.

- (c): Removing the acknowledgments would speed up the application, as it
would lose the overhead of communication between the processes. Although, if
the number of tasks is really big and the receiver takes a long time to
finish each one, the accumulation of tasks on the receiver's mailbox could
create a memory problem.

- (d): In both scenarios, the performance would get better, because it would
mean less idle time for the receiver, but the possible downside outlined in
(c) is still possible.

- (e): In both scenarios, the long time that it would take for the work
creation would mean a lot of idle time for the receiver which hinders
performance.

## Problem 5

- (a):

    ```Haskell
    map_reduce :: (k -> v -> [(k2, v2)])     -- mapper
               -> (k2 -> [v2] -> [(k2, v2)]) -- reducer
               -> [(k, v)] -> [(k2, v2)]
    ```

- (b):

    ```Erlang
    map(_, []) ->
        [];
    map(Page, [W|Ws]) ->
        [{W, Page} | map(Page, Ws)].
    
    reduce(W, Pages) ->
        [{W, Pages}].
    ```

- (c):

    ```Erlang
    map(_, Words) ->
        map(Words).
    map([]) ->
        [];
    map([W | Ws]) ->
        [{W, found} | map(Ws)].
    
    reduce(W, Occs) ->
        [{W, length(Occs)}].
    ```

- (d):

  - (i): Replicated files are files shared between nodes, which make them
  able to survive crashes. Local files are only stored on one node and are
  lost on the event of a crash.

  - (ii): Local files

  - (iii): Replicated files

  - (iv): If a worker node crashes the master node reruns its entire work on
  another node. Even if the worker had finished its job, the results are
  stored in local files, so its whole work will have to be redone.

## Problem 6

- (a):

  - (i): Threaded code means that in the VM, the calls and returns of the
  functions are simple gotos.
  
  - (ii): Because of data locality.

- (b): The circuit breaker pattern is a fault tolerance mechanism used in
distributed systems. Its use is to monitor communication between other
processes, detect failures and stop the issuing of messages towards failing
processes to avoid overloading the system.

- (c): A network partition is when a distributed system has been divided into
several small subnets that communicate with each other, and the communication
between two or more of these subnets stops working.

- (d):

  - (i): Consistency, Availability, Partition tolerance

  - (ii): The theorem says that in distributed networks we have to take for
  granted that paritioning will happen and choose between consistency and
  availability to offer. We can't have all 3 of the CAP together.

  - (iii): For example, if a code sharing service loses connection to one
  of its databases through partitioning where a code file is stored, the
  system will have to drop availability until it gets fixed. In this setting,
  dropping consistency is not an option, since we don't want the system to
  "guess" and fill in the missing file without being sure every character is
  as it should be.

- (e):

## Problem 7

- (a): A transaction is a group of related actions that need to be performed
as a single action.

- (b):

  - (i): If one of the accounts does not have enough funds to transfer, the
  thread will block due to the retry.

  - (ii): m

- (c):

  - (i):

    ``` Haskell
    newtype Lock = Lock (TVar Int)

    claim :: Lock -> STM ()
    claim (Lock shared) = atomically $ do
        x <- readTVar shared
        if x == 0 retry
        else return ()

    release :: Lock -> STM ()
    release (Lock shared) = atomically $ do
        writeTVar shared 1
        return ()
    ```

  - (ii): If the first thread claims lockA, then the second thread claims
  lockB and then both try waiting for the other, this will lead to a
  deadlock.

  - (iii): This is a correct way of writing it, since if a thread claims one
  lock it will have to also claim the second lock before the other thread
  has any chance of running.
